---
title: "Stats 101"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: scroll
runtime: shiny
---

```{r setup, include=FALSE}
library(dplyr)
library(tidyr)
library(shiny)
library(DT)
library(flexdashboard)
library(ggplot2)
library(ggthemes)
library(hrbrthemes)
library(googlesheets)
library(skimr)
library(modelr)
library(ggbeeswarm)
library(forcats)
gs_auth("googlesheets_token.rds")

getdata <- function() {
  sheet<-gs_title("About you (Responses)")
  df<-gs_read(sheet) 
  names(df)<-c("id","focus", "age", "happy","income")
  df<- df %>% mutate(focus=fct_lump(focus,3),
                    income=fct_lump(income,3),
                     age=pmin(age,70)) %>% 
    filter(!is.na(age))
 # df<- tidyr::separate(df,social)
  return(df)
}

```


```{r}
results <- reactive({
  invalidateLater(30000, session)
  getdata()
  }
)

```

Intro
===================================== 


Welcome!

Please fill out [bit.ly/stats101survey](bit.ly/stats101survey)

Agenda

- Gathering data
- Exploring it
- Comparing groups
- Predicting values

About Me

- Own a data science consultancy
- Microsoft Data Platform MVP
- Lots of community stuff
- itsalocke.com
- @stefflocke @lockedata

![](https://itsalocke.com/img/ChibiSteph.svg)


Data
=====================================  
All data
-----------------------------------------------------------------------
### Records
```{r}
renderValueBox({
  p<-nrow(results())
  valueBox(p,caption = "Records", icon="fa-hashtag",color = "primary")
})
```

### Table

```{r}
renderDataTable({results()})
```

Tables
=====================================  

- **Mean** is the sum of all values divided by the number of values
- **Standard deviation** is how far away you have to go from the mean to get most values

### Normal

```{r}
 ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + ylab("") +
  scale_y_continuous(breaks = NULL)+ 
    hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    )
```

### Happy 

```{r}
renderDataTable({
  results() %>% 
    group_by(happy) %>% 
    summarise(n=n(),median =median(age),mean = mean(age), sd=sd(age), min=min(age), max=max(age))
}, options = list(
  bPaginate = FALSE
))
```

### Salary

```{r}
renderDataTable({
  results() %>% 
    group_by(income) %>% 
    summarise(n=n(), mean = mean(age), sd=sd(age), min=min(age), max=max(age))
}, options = list(
  bPaginate = FALSE
))
```

### Focus

```{r}
renderDataTable({
  results() %>% 
    group_by(focus) %>% 
    summarise(n=n(), mean = mean(age), sd=sd(age), min=min(age), max=max(age))
}, options = list(
  bPaginate = FALSE
))
```

Charts
=====================================  

Age
-----------------------------------------------------------------------
### Happy with job 1
```{r}
renderPlot({
  ggplot(results(), aes(x = age, colour= happy, group=happy)) +
    geom_density() + 
    hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    ) +
    labs(x = "Happy", y = "Age",
         title = "Age distribution  by happiness") 
})
```

### Happy with job 2
```{r}
renderPlot({
  ggplot(results(), aes(x = happy, y= age)) +
    geom_boxplot(alpha=0, colour="grey")+
    geom_beeswarm(colour = "#2165B6", size =3) + 
    hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    ) +
    labs(x = "Happy", y = "Age",
         title = "Age distribution  by happiness") 
})
```

<!--
### Location
```{r}
renderPlot({
  ggplot(results(), aes(x = continent, y= age)) +
    geom_boxplot(alpha=0, colour="grey")+
    geom_beeswarm(colour = "#2165B6", size =3) + 
    hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    ) +
    labs(x = "Location", y = "Age",
         title = "Age distribution  by location") 
})
```
-->

### Focus 1
```{r}
renderPlot({
  ggplot(results(), aes(x = age, colour= focus, group=focus)) +
    geom_density() + 
    hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    ) +
    labs(x = "Happy", y = "Age",
         title = "Age distribution  by focus") 
})
```

### Focus 2
```{r}
renderPlot({
  ggplot(results(), aes(x = focus, y= age)) +
    geom_boxplot(alpha=0, colour="grey")+
    geom_beeswarm(colour = "#2165B6", size =3) + 
    hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    ) +
    labs(x = "Focus", y = "Age",
         title = "Age distribution by focus") 
})
```

### Income 1
```{r}
renderPlot({
  ggplot(results(), aes(x = age, colour= income, group=income)) +
    geom_density() + 
    hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    ) +
    labs(x = "Happy", y = "Age",
         title = "Age distribution  by focus") 
})
```

### Income 2
```{r}
renderPlot({
  ggplot(results(), aes(x = income, y= age)) +
    geom_boxplot(alpha=0, colour="grey")+
    geom_beeswarm(colour = "#2165B6", size =3) + 
    hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    ) +
    labs(x = "Focus", y = "Age",
         title = "Age distribution by focus") 
})
```

Are happy people older?
=====================================  

Job satisfaction
-----------------------------------------------------------------------
Hypothesis: People who are unhappy with their job are younger than those who are happy with their job.

A t-test takes two groups and determines how likely it is for the two values to be generated from the same data i.e. the two groups are not actually different. The smaller the result, the less likely the two groups are actually one group. You should set a threshold before hand - commonly 0.05!

Run a t-test in R using the `t.test()` function.


### Age of happy people
```{r}
renderValueBox({
  results() %>% 
    filter( happy == "Yes") %>% 
    summarise(mean=mean(age)) ->
    yes.mean
  p<-round(yes.mean$mean,1)
  valueBox(p,caption = "Mean age of happy people", icon="fa-user",color = "primary")
})
```

### Age of unhappy people
```{r}
renderValueBox({
  results() %>% 
    filter( happy == "No") %>% 
    summarise(mean=mean(age)) ->
    no.mean
  p<-round(no.mean$mean,1)
  valueBox(p,caption = "Mean age of unhappy people", icon="fa-user",color = "warning")
})
```

### Are the mean's *really* different?
```{r}
renderValueBox({
  p<-round(t.test(age ~ happy, results())$p.value , 3)
  valueBox(p,caption = "Happiness T-Test", icon="fa-asterisk",color = ifelse(p<=0.05,"primary","danger"))
})
```

Can we predict age?
=====================================  

### Setup
Challenge: Based on the information provided, can we predict a person's age?

What sort of things do we need to consider first?

- Do we have sufficient volumes of data?
- Is there extra information we can use?
- Is there bias that will be reflected in our results?
- Are there data quality issues or changes to data capture that could impact results?

Then what?

- What types of models could fit our challenge?
- How will we evaluate whether we've got a good model?
- How are we going to test our predictions?

### Sampling

We typically take some **training** data for building out model and some **testing** data for checking whether it works.

```{r}
sliderInput("sampleper", "Training percent",min = 0, max=100,value = 70, step =1)
              
```

```{r}
set.seed(888)
sampled<-reactive({
  tr<-input$sampleper/100
  te<-1-tr
  resample_partition(results(),
                    c(train = tr,test = te ))
})
```

### Training size
```{r}
renderValueBox({
  p<-nrow(sampled()$train)
  valueBox(p,caption = "Training", icon="fa-hashtag")
})
```

### Testing size
```{r}
renderValueBox({
  p<-nrow(sampled()$test)
  valueBox(p,caption = "Testing", icon="fa-hashtag")
})
```

### Candidate models
We have a continuous variable that we're trying to predict. We have a lot of categorical variable inputs.

The simplest model would be using the average -- whether **mean** or **median**.

We can do a line of best fit model (known as linear regression).

[setosa.io](http://setosa.io/ev/ordinary-least-squares-regression/)

Another option is a decision tree since we have lots of categorical inputs.

[r2d3.us](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)

### Using simple measures
```{r}
training<-reactive({as_data_frame(sampled()$train)})
testing<-reactive({as_data_frame(sampled()$test)})
```

The median age is `r reactive({median(training()$age)})`.

The mean age is `r reactive({round(mean(training()$age),1)})`.

```{r}
renderPlot({
qplot(data=training(), x=age) + geom_vline(aes(xintercept = median(training()$age)), colour="blue")+ geom_vline(aes(xintercept = mean(training()$age)),colour="red")+
    hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    )
})
```

```{r}
renderPlot({
qplot(data=training(), x=age) + geom_vline(aes(xintercept = median(training()$age)), colour="blue")+ geom_vline(aes(xintercept = mean(training()$age)),colour="red")+
    hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    )
})
```

### Using a line of best fit
If we used one variable to predict age, what would our predictions look like?

### Happy
`lm(age~happy, data=training)`
```{r}
happymodel<-reactive({lm(age~happy, data=training())})

```

```{r}

renderPlot({
  happymodel<-lm(age~happy, data=training())
training() %>% 
  group_by(happy) %>%
  summarise(n=n()) %>% 
  add_predictions(happymodel, "age") ->
  preds
ggplot(training(), aes(x=happy, y=age))+
  geom_point()+
  geom_point(data=preds, colour="blue", size=5)+
   hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    )+
    labs(x = "Happy", y = "Age",
         title = "Age distribution  by happiness") 
})
```

### Focus
`lm(age~focus, data=training)`

```{r}

renderPlot({
  focusmodel<-lm(age~focus, data=training())
training() %>% 
  group_by(focus) %>%
  summarise(n=n()) %>% 
  add_predictions(focusmodel, "age") ->
  preds
ggplot(training(), aes(x=focus, y=age))+
  geom_point()+
  geom_point(data=preds, colour="blue", size=5)+
   hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    )+
    labs(x = "Focus", y = "Age",
         title = "Age distribution  by focus") 
})
```

### Income
`lm(age~income, data=training)`

```{r}

renderPlot({
  incomemodel<-lm(age~income, data=training())
training() %>% 
  group_by(income) %>%
  summarise(n=n()) %>% 
  add_predictions(incomemodel, "age") ->
  preds
ggplot(training(), aes(x=income, y=age))+
  geom_point()+
  geom_point(data=preds, colour="blue", size=5)+
   hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    )+
    labs(x = "Income", y = "Age",
         title = "Age distribution  by focus") 
})
```

### All the things
Combining multiple variables

`lm(age~focus + age + income, data=training)`

```{r}

renderPlot({
  fullmodel<-lm(age~focus + happy + income, data=training())
training() %>% 
  group_by(focus, happy, income) %>%
  summarise(n=n()) %>% 
  add_predictions(fullmodel, "age") ->
  preds
ggplot(training(), aes(x=age, y=age))+
  geom_point()+
  geom_point(data=preds, colour="blue", size=5)+
   hrbrthemes::theme_ipsum(
      base_size = 18,
      grid = FALSE,
      axis = FALSE
    )+
    labs(x = "Age", y = "Age",
         title = "Age distribution") 
})
```

### Picking a model
You can pick models by different metrics based on your business success criteria. Here we're going to use the Mean Squared Error (MSE) after making predictions on our test data. Whichever one has the smallest value, that is our most accurate model and we'll use that going forward.

Mean Squared Error is the average value of all the squared differences betweeen predicted and actual values. We square them to make everything positive and to penalise larger errors.

```{r}
renderDataTable({
  
  happymodel<-lm(age~happy, data=training())
  focusmodel<-lm(age~focus, data=training())
  incomemodel<-lm(age~income, data=training())
  fullmodel<-lm(age~happy+focus+income, data=training())
  
 testing() %>% 
  add_predictions(happymodel, "happymodel")%>% 
  add_predictions(focusmodel, "focusmodel")%>% 
  add_predictions(incomemodel, "incomemodel")%>% 
  add_predictions(fullmodel, "fullmodel")%>% 
  mutate(medianmodel=median(training()$age),
         meanmodel=round(mean(training()$age),1)) %>% 
  gather(model, pred_age, ends_with("model")) %>% 
  mutate(square_error=(pred_age-age)^2) %>% 
  group_by(model) %>% 
  summarise(mean(square_error))
}, options = list(
  bPaginate = FALSE
))
```
